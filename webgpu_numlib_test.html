<!DOCTYPE html>
<html>
<head><title>Testing webgpu numlib</title>

<script src="./app.js" type="module"></script>



</head>
<body>
<h1>Testing webgpu for Walnut 2.0</h1>
<p>Note that because of CORS issues, you must run this file from a server; running from file 
   system will not work. See console for outputs (press F12 on Windows).</p>
<p>To run this (early 2020), you must use 
<a href="https://www.google.com/chrome/canary/" target="_blank">Chrome Canary</a> and enable the following flag:</p>
<pre>chrome://flags/#enable-unsafe-webgpu</pre>
<p>Or use another experimental browser ('nightly') that supports this.</p>
<p>This file is based on <a href="https://developers.google.com/web/updates/2019/08/get-started-with-gpu-compute-on-the-web" 
    target="_blank">this excellent tutorial</a>.</p>
<p>On a laptop with only 24 graphics units, multiplying a 1000x1000 with itself, takes about 10 s 
with gpu, whereas cpu takes about 40 to 45 s. With a 1500x1500 matrix, the gpu
takes about 45 s and the cpu takes about 160 s. With 1500 and 2000 there is a chance the browser
will shift to debug mode or crash on a modest laptop.
</p>
<p>
    Looking at the time it takes to process the to-be-iterated portion of the code, without
    the array unpacking and GLSL compiling, which needs to be done only once, we see that it
    takes about 2.2 s, including reading the result back to CPU. This suggest a 20-fold 
    speed-up with GPU (with only 24 GPU shader compute cores). 
    However, we may be able to optimize the CPU matrix multiplication,
    which we have not done yet. 
</p>
</body>
</html>
